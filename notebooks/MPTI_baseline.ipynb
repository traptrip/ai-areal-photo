{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9Sp_jegNHBxv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "from PIL import Image\n",
        "from PIL.ExifTags import TAGS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"mps\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHAkflGFZaWS"
      },
      "source": [
        "## Датасет\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fZnd_HZ1ZYgK"
      },
      "outputs": [],
      "source": [
        "from math import sin, cos\n",
        "\n",
        "json_dir = \"../data/train/json/\"\n",
        "\n",
        "\n",
        "data_df = pd.DataFrame({'id': [], \"left_top_x\": [], 'left_top_y': [], \"right_bottom_x\": [], 'right_bottom_y': [], 'angle': []})\n",
        "\n",
        "json_true = []\n",
        "rows = []\n",
        "for _, _, files in os.walk(json_dir):\n",
        "  for x in files:\n",
        "    if x.endswith(\".json\"):\n",
        "      data = json.load(open(json_dir + x))\n",
        "      new_row = {\n",
        "        'id':x.split(\".\")[0]+\".img\", \n",
        "        'left_top_x':data[\"left_top\"][0], \n",
        "        'left_top_y':data[\"left_top\"][1], \n",
        "        'right_bottom_x': data[\"right_bottom\"][0], \n",
        "        \"right_bottom_y\": data[\"right_bottom\"][1], \n",
        "        'angle': data[\"angle\"]\n",
        "    }\n",
        "      rows.append(new_row)\n",
        "\n",
        "data_df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "juoL5gfL57hz",
        "outputId": "1674bae5-5d8f-4eb8-f8dc-3c10ce642175"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>left_top_x</th>\n",
              "      <th>left_top_y</th>\n",
              "      <th>right_bottom_x</th>\n",
              "      <th>right_bottom_y</th>\n",
              "      <th>angle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>729.img</td>\n",
              "      <td>7468</td>\n",
              "      <td>8270</td>\n",
              "      <td>6267</td>\n",
              "      <td>7461</td>\n",
              "      <td>169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>683.img</td>\n",
              "      <td>4829</td>\n",
              "      <td>2951</td>\n",
              "      <td>4240</td>\n",
              "      <td>4274</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>379.img</td>\n",
              "      <td>8319</td>\n",
              "      <td>8520</td>\n",
              "      <td>7006</td>\n",
              "      <td>9133</td>\n",
              "      <td>110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>396.img</td>\n",
              "      <td>4882</td>\n",
              "      <td>5854</td>\n",
              "      <td>6023</td>\n",
              "      <td>6745</td>\n",
              "      <td>353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>816.img</td>\n",
              "      <td>8764</td>\n",
              "      <td>2818</td>\n",
              "      <td>9889</td>\n",
              "      <td>1907</td>\n",
              "      <td>276</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  left_top_x  left_top_y  right_bottom_x  right_bottom_y  angle\n",
              "0  729.img        7468        8270            6267            7461    169\n",
              "1  683.img        4829        2951            4240            4274     69\n",
              "2  379.img        8319        8520            7006            9133    110\n",
              "3  396.img        4882        5854            6023            6745    353\n",
              "4  816.img        8764        2818            9889            1907    276"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df.to_csv(\"../data/train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/and/miniforge3/envs/mipt-hack/lib/python3.9/site-packages/PIL/Image.py:3035: DecompressionBombWarning: Image size (110166016 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "image = Image.open(\"/Users/and/projects/hacks/hacks_ai_aerial_photo/data/original.tiff\")\n",
        "exifdata = image.getexif()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "tags = []\n",
        "data = []\n",
        "for tag_id in exifdata:\n",
        "    # get the tag name, instead of human unreadable tag id\n",
        "    tag = TAGS.get(tag_id, tag_id)\n",
        "    data.append(exifdata.get(tag_id))\n",
        "    # decode bytes \n",
        "    if isinstance(data[-1], bytes):\n",
        "        data[-1] = data[-1].decode()\n",
        "    tags.append(tag)\n",
        "    # print(f\"{tag:25}: {data}\")\n",
        "\n",
        "print(\"TAGS\")\n",
        "for tag in tags:\n",
        "    print(tag)\n",
        "print()\n",
        "\n",
        "for tag, d in zip(tags, data):\n",
        "    print(f\"{tag:25}: {d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Nxzkp2kCLz5F"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_df, transform=None):\n",
        "\n",
        "        self.data_df = data_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # достаем имя изображения и ее лейбл\n",
        "        image_name, labels = self.data_df.iloc[idx]['id'], [self.data_df.iloc[idx]['left_top_x'], self.data_df.iloc[idx]['left_top_y'],self.data_df.iloc[idx]['right_bottom_x'],self.data_df.iloc[idx]['right_bottom_y'], self.data_df.iloc[idx]['angle']]\n",
        "\n",
        "        # читаем картинку. read the image\n",
        "        image = cv2.imread(f\"/content/train/{image_name}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(image)\n",
        "        \n",
        "        # преобразуем, если нужно. transform it, if necessary\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, torch.tensor(labels).long()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WaERrZF_MB0T"
      },
      "outputs": [],
      "source": [
        "# задаем преобразование изображения.\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2GXPia6kMqZY"
      },
      "outputs": [],
      "source": [
        "# читаем датасет\n",
        "data_df = pd.read_csv(\"../data/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "3iNjkKMXM2Nw",
        "outputId": "0118cd90-05b1-4141-aae9-611cf84ee17d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Обучающей выборки  800\n",
            "Тестовой выборки  400\n"
          ]
        }
      ],
      "source": [
        "from os import listdir\n",
        "\n",
        "print(\"Обучающей выборки \" ,len(listdir(\"../data/train/img\")))\n",
        "print(\"Тестовой выборки \" ,len(listdir(\"../data/test\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2sKp7D0MM3yH"
      },
      "outputs": [],
      "source": [
        "# разделим датасет на трейн и валидацию, чтобы смотреть на качество\n",
        "train_df, valid_df = train_test_split(data_df, test_size=0.2, random_state=43)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMS7A945M-s8",
        "outputId": "71bfd775-e7df-41ff-92e8-748a03e7df78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((640, 6), (160, 6))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.shape, valid_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ItT6LNTXM_9n"
      },
      "outputs": [],
      "source": [
        "train_dataset = ImageDataset(train_df, train_transform)\n",
        "valid_dataset = ImageDataset(valid_df, valid_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "R3iY0yesNCPr"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=32,\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True,\n",
        "                                           num_workers=4)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                           batch_size=32,\n",
        "                                           # shuffle=True,\n",
        "                                           pin_memory=True,\n",
        "                                           num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN1MdlB5NFch"
      },
      "source": [
        "## Вспомогательные функции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "w4hQMzkXNDXZ"
      },
      "outputs": [],
      "source": [
        "def crossvalid(res_model=None, criterion=None, optimizer=None, dataset=None, k_fold=5):\n",
        "    \n",
        "    train_score = pd.Series()\n",
        "    val_score = pd.Series()\n",
        "    \n",
        "    total_size = len(dataset)\n",
        "    fraction = 1 / k_fold\n",
        "    seg = int(total_size * fraction)\n",
        "    # tr:train,val:valid; r:right,l:left;  eg: trrr: right index of right side train subset \n",
        "    # index: [trll,trlr],[vall,valr],[trrl,trrr]\n",
        "    for i in range(k_fold):\n",
        "        trll = 0\n",
        "        trlr = i * seg\n",
        "        vall = trlr\n",
        "        valr = i * seg + seg\n",
        "        trrl = valr\n",
        "        trrr = total_size\n",
        "        \n",
        "        train_left_indices = list(range(trll,trlr))\n",
        "        train_right_indices = list(range(trrl,trrr))\n",
        "        \n",
        "        train_indices = train_left_indices + train_right_indices\n",
        "        val_indices = list(range(vall,valr))\n",
        "        \n",
        "        train_set = torch.utils.data.dataset.Subset(dataset,train_indices)\n",
        "        val_set = torch.utils.data.dataset.Subset(dataset,val_indices)\n",
        "        \n",
        "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=50,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=50,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "        train_acc = train(res_model,criterion,optimizer,train_loader,val_loader,1)\n",
        "        train_score.at[i] = train_acc\n",
        "        #val_acc = valid(res_model,criterion,optimizer,val_loader)\n",
        "        #val_score.at[i] = val_acc\n",
        "    \n",
        "    return train_score,val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "l9ZMw7IhNHTc"
      },
      "outputs": [],
      "source": [
        "def plot_history(train_history, val_history, title='loss'):\n",
        "    plt.figure()\n",
        "    plt.title('{}'.format(title))\n",
        "    plt.plot(train_history, label='train', zorder=1)\n",
        "    \n",
        "    points = np.array(val_history)\n",
        "    steps = list(range(0, len(train_history) + 1, int(len(train_history) / len(val_history))))[1:]\n",
        "    \n",
        "    plt.scatter(steps, val_history, marker='+', s=180, c='orange', label='val', zorder=2)\n",
        "    plt.xlabel('train steps')\n",
        "    \n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3yZV_C2gNJgx"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, train_dataloader, test_dataloader, NUM_EPOCH=15):\n",
        "    train_loss_log = []\n",
        "    val_loss_log = []\n",
        "    \n",
        "    train_acc_log = []\n",
        "    val_acc_log = []\n",
        "    \n",
        "    for epoch in tqdm(range(NUM_EPOCH)):\n",
        "        model.train()\n",
        "        train_loss = 0.\n",
        "        train_size = 0\n",
        "        \n",
        "        train_pred = 0.\n",
        "\n",
        "        for imgs, labels in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            y_pred = model(imgs)\n",
        "\n",
        "            loss = criterion(y_pred, labels)\n",
        "            loss.backward()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            train_size += y_pred.size(0)\n",
        "            train_loss_log.append(loss.data / y_pred.size(0))\n",
        "            \n",
        "            train_pred += (y_pred.argmax(1) == labels).sum()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        train_acc_log.append(train_pred / train_size)\n",
        "\n",
        "        val_loss = 0.\n",
        "        val_size = 0\n",
        "        \n",
        "        val_pred = 0.\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in test_dataloader:\n",
        "                \n",
        "                imgs = imgs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                pred = model(imgs)\n",
        "                loss = criterion(pred, labels)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                val_size += pred.size(0)\n",
        "                \n",
        "                val_pred += (pred.argmax(1) == labels).sum()\n",
        "        \n",
        "\n",
        "        val_loss_log.append(val_loss / val_size)\n",
        "        val_acc_log.append(val_pred / val_size)\n",
        "\n",
        "        clear_output()\n",
        "        plot_history(train_loss_log, val_loss_log, 'loss')\n",
        "        \n",
        "\n",
        "\n",
        "        print('Train loss:', (train_loss / train_size)*100)\n",
        "        print('Val loss:', (val_loss / val_size)*100)\n",
        "        print('Train acc:', (train_pred / train_size)*100)\n",
        "        print('Val acc:', (val_pred / val_size)*100)\n",
        "        \n",
        "    return train_loss_log, train_acc_log, val_loss_log, val_acc_log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqf49DIjNNBs"
      },
      "source": [
        "## Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jPrZ0ESGNKzo"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hc7cZmckNPDD"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The MPS backend is supported on MacOS 12.3+.Current OS version can be queried using `sw_vers`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/Users/and/projects/hacks/hacks_ai_aerial_photo/notebooks/MPTI_baseline.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/and/projects/hacks/hacks_ai_aerial_photo/notebooks/MPTI_baseline.ipynb#ch0000018?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mresnet50(weights\u001b[39m=\u001b[39mmodels\u001b[39m.\u001b[39mResNet50_Weights\u001b[39m.\u001b[39mIMAGENET1K_V2)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/and/projects/hacks/hacks_ai_aerial_photo/notebooks/MPTI_baseline.ipynb#ch0000018?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mfc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(model\u001b[39m.\u001b[39mfc\u001b[39m.\u001b[39min_features, \u001b[39m3\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/and/projects/hacks/hacks_ai_aerial_photo/notebooks/MPTI_baseline.ipynb#ch0000018?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/and/projects/hacks/hacks_ai_aerial_photo/notebooks/MPTI_baseline.ipynb#ch0000018?line=6'>7</a>\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss()\n",
            "File \u001b[0;32m~/miniforge3/envs/mipt-hack/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
            "File \u001b[0;32m~/miniforge3/envs/mipt-hack/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/mipt-hack/lib/python3.9/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/miniforge3/envs/mipt-hack/lib/python3.9/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The MPS backend is supported on MacOS 12.3+.Current OS version can be queried using `sw_vers`"
          ]
        }
      ],
      "source": [
        "# Подргружаем модель\n",
        "\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "model.fc = nn.Linear(model.fc.in_features, 3)\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgziL-TRNPFL"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SU4esQvNPHs"
      },
      "outputs": [],
      "source": [
        "train_loss_log, train_acc_log, val_loss_log, val_acc_log = train(model, \n",
        "                                                                 criterion, \n",
        "                                                                optimizer, \n",
        "                                                                 train_loader, \n",
        "                                                                 valid_loader, \n",
        "                                                                 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipPvpzg5NgNY"
      },
      "source": [
        "## Посчитаем метрику"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMMUY_YrN1Vg"
      },
      "outputs": [],
      "source": [
        "def compute_metric(data_true, data_pred, outImageW = 10496, outImageH = 10496):\n",
        "\n",
        "  x_center_true = int((data_true[\"left_top_x\"] + data_true[\"right_bottom_x\"])/2)\n",
        "  y_center_true = int((data_true[\"left_top_y\"] + data_true[\"right_bottom_y\"])/2)\n",
        "\n",
        "  x_metr = x_center_true - int((data_pred[\"left_top_x\"] + data_pred[\"right_bottom_x\"])/2)\n",
        "  y_metr = y_center_true - int((data_pred[\"left_top_y\"] + data_pred[\"right_bottom_y\"])/2) \n",
        "\n",
        "  metr =  1- 0.7 * (abs(x_metr)/outImageH + abs(y_metr)/outImageW)/2 + 0.3 *abs(data_true[\"angle\"] - data_pred[\"angle\"])/359\n",
        "  return metr"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "МФТИ_baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('mipt-hack')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ee6ca8987acf3715f4f48c25060230bff18e56c786c03a60785b9f9bbf3e5936"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
